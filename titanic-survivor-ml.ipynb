{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Titanic Survivors Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.nationalgeographic.org/assets/photos/000/273/27302.jpg\" height=\"300\" width=\"450\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8eb8a08a-2d8f-44d5-88bd-f89e6ad1db3b",
    "_uuid": "c9e96fa9-0bb6-4f59-8f8c-c1c53fab35a9"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/titanic/train.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.value_counts(df['Pclass']).plot.bar()\n",
    "plt.xlabel('Proxy for Socio-Economic Status (SES)')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['Sex']).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['Age'] , edgecolor='black', linewidth=1.2)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder() \n",
    "\n",
    "df['Sex_category'] = label_encoder.fit_transform(df['Sex'])\n",
    "\n",
    "df['Embarked'].fillna('None', inplace=True)\n",
    "\n",
    "df['Embarked_category'] = label_encoder.fit_transform(df['Embarked'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassengerId looks like it has very little impact on any of the other variables within the model. Let's drop it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='PassengerId', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df['Cabin'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not sure how relevant the cabin on the ship will be to survival. Aside from that though it only has a 22.9% non-null population.  \n",
    "### We'll also drop this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Cabin', 'Ticket', 'Fare', 'Name'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's run this again now that we've narrowed down to our selected features\n",
    "plt.figure(figsize=(12,10))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_category', 'Embarked_category']]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will split the training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#I think for this comp, the train/test csv's are already the splitting but we might want to look into using the entirity of one to accomplish their respective namesakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#instantiate\n",
    "RFC = RandomForestClassifier(n_estimators=100).fit(X_train, y_train) #start w/all default values\n",
    "RFC2 = RandomForestClassifier(n_estimators=10).fit(X_train, y_train) #start w/all default values\n",
    "#fit\n",
    "\n",
    "trainscore = RFC.score(X_train, y_train)\n",
    "testscore = RFC.score(X_test, y_test)\n",
    "\n",
    "trainscore2 = RFC2.score(X_train, y_train)\n",
    "testscore2 = RFC2.score(X_test, y_test)\n",
    "\n",
    "print(f'Accuracy of Random Forest Classifier algorithm on training set (100 estimators): {trainscore}')\n",
    "#predict\n",
    "print(f'Accuracy of Random Forest Classifier algorithm on test set (100 estimators): {testscore}')\n",
    "\n",
    "print(f'Accuracy of Random Forest Classifier algorithm on training set (10 estimators): {trainscore2}')\n",
    "#predict\n",
    "print(f'Accuracy of Random Forest Classifier algorithm on test set (10 estimators): {testscore2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "testdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['Sex_category'] = label_encoder.fit_transform(testdf['Sex'])\n",
    "\n",
    "testdf['Embarked'].fillna('None', inplace=True)\n",
    "\n",
    "testdf['Embarked_category'] = label_encoder.fit_transform(testdf['Embarked'])\n",
    "\n",
    "testdf['Age'].fillna(df['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testdf[['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_category', 'Embarked_category']]\n",
    "\n",
    "testdf['Survived'] = RFC.predict(testX)\n",
    "\n",
    "testdf[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading = pd.read_csv(\"../input/titanic/gender_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in testdf['Survived']:\n",
    "    if testdf['Survived'][i] == grading['Survived'][i]:\n",
    "        correct+=1\n",
    "    else:\n",
    "        incorrect+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You have correctly predicted \" + str(correct/(correct+incorrect))+ \" survivors in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "### So. We have a big overfitting problem. Our training set has a 94% accuracy which reduces to 81% on the test set. When we introduce the prediction data set without the survival column, we have even lower accuracy at 61%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate accuracy and maybe perform cross validation - gridsearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3], \n",
    "    'min_samples_leaf': [3, 4, 5], \n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "griddy = GridSearchCV(estimator=RFC,\n",
    "             param_grid=params,\n",
    "            cv = 3, \n",
    "            verbose = 2)\n",
    "\n",
    "griddy.fit(X_train, y_train)\n",
    "\n",
    "griddy.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFCbest = griddy.best_estimator_\n",
    "\n",
    "#griddy_acc = evaluate(best_grid, X_test, Y_test)\n",
    "\n",
    "griddy_acc = RFCbest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Base Accuracy (no parameter tuning) on training set: {trainscore}')\n",
    "#predict\n",
    "print(f'Base Accuracy (no parameter tuning) on test set: {testscore}')\n",
    "\n",
    "print(f'Accuracy after GridSearchCV on test set: {griddy_acc}')\n",
    "\n",
    "print(f'Gain of accuracy: {(griddy_acc-testscore)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try this hyper-tuned Random Forest Algorithm on our prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['SurvivedBest'] = RFCbest.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf[['SurvivedBest', 'Survived']]\n",
    "\n",
    "testdf.to_csv(r'C:\\Users\\LIDFS61\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading[['Survived']]\n",
    "\n",
    "grading.to_csv(r'C:\\Users\\LIDFS61\\grading.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctpreds=0\n",
    "incorrectpreds=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in testdf['SurvivedBest']:\n",
    "    if testdf['SurvivedBest'][i] == grading['Survived'][i]:\n",
    "        correctpreds+=1\n",
    "    else:\n",
    "        incorrectpreds+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['SurvivedBest'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading['Survived'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['SurvivedBest'][33] == grading['Survived'][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You have correctly predicted \" + str(correctpreds/(correctpreds+incorrectpreds)*100) + \"% of the survivors in the dataset.\")\n",
    "print(\"This is a gain of \" + str((correctpreds/(correctpreds+incorrectpreds)) - (correct/(correct+incorrect))) + \" accuracy.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
